{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('notaphoenix/shakespeare_dataset')\n",
    "\n",
    "# For convenience we use the training data as a list of str:\n",
    "corpus = [elt['text'] for elt in dataset['training']]\n",
    "#tokenized_corpus = [list(word) for word in corpus] # one-liner (optional)\n",
    "\n",
    "tokenized_corpus = []\n",
    "for elt in corpus:\n",
    "    tokenized_corpus.append(list(elt))\n",
    "\n",
    "tokenized_corpus[0]\n",
    "\n",
    "#vocab = {char: i for i, char in enumerate(set(\"\".join(corpus)))} # one-liner (optional)\n",
    "\n",
    "vocab = {}\n",
    "for elt in tokenized_corpus: # iterate over corpus\n",
    "    for char in elt: # iterate over characters of elt\n",
    "        if char not in vocab: # add to dictionary\n",
    "            vocab[char] = len(vocab) # index of character is current length of vocab !\n",
    "\n",
    "vocab.items()\n",
    "\n",
    "def get_pairs(tokenized_corpus):\n",
    "    \"\"\"Get the frequency of adjacent pairs in the words.\"\"\"\n",
    "    pairs = {}\n",
    "    for word in tokenized_corpus:\n",
    "        for i in range(len(word) - 1):\n",
    "            if (word[i], word[i + 1]) not in pairs:\n",
    "                pairs[(word[i], word[i + 1])] = 1\n",
    "            else:\n",
    "                pairs[(word[i], word[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(pair, tokenized_corpus):\n",
    "    \"\"\"Merge the most frequent pair in all tokenized_corpus.\"\"\"\n",
    "    new_words = []\n",
    "    for word in tokenized_corpus:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                new_word.append(word[i] + word[i + 1])  # Merge pair (\"+\" is str concatenation)\n",
    "                i += 2  # Skip next character since it's merged\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def byte_pair_encoding(corpus, num_merges: int = 10):\n",
    "    \"\"\"Perform BPE on a given corpus.\"\"\"\n",
    "    tokenized_corpus = [list(word) for word in corpus]  # Start with character tokens\n",
    "    vocab = {char: i for i, char in enumerate(set(\"\".join(corpus)))}  # Initial vocab\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_pairs(tokenized_corpus)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        tokenized_corpus = merge_pair(best_pair, tokenized_corpus)\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[new_token] = len(vocab)  # Assign new token an index\n",
    "    \n",
    "    return vocab, tokenized_corpus\n",
    "\n",
    "vocab, tokenized_corpus = byte_pair_encoding(corpus, num_merges=500)\n",
    "\n",
    "def tokenize_to_str_list(s: str, vocab):\n",
    "    \"\"\"Tokenize a given string based on the trained BPE vocabulary.\"\"\"\n",
    "    tokens = list(s)  # Start with character tokens\n",
    "    \n",
    "    while True:\n",
    "        pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "        valid_pairs = [pair for pair in pairs if pair[0] + pair[1] in vocab]\n",
    "        \n",
    "        if not valid_pairs:\n",
    "            break\n",
    "        \n",
    "        best_pair = max(valid_pairs, key=lambda p: vocab.get(p[0] + p[1], float('-inf')))\n",
    "        merged_token = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                new_tokens.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        tokens = new_tokens\n",
    "    \n",
    "    return tokens    \n",
    "\n",
    "tokenize_to_str_list(dataset['test'][0]['text'], vocab)\n",
    "\n",
    "def tokenize(s: str, vocab):\n",
    "    return [vocab[elt] for elt in tokenize_to_str_list(s, vocab)]\n",
    "\n",
    "tokenize(dataset['training'][0]['text'], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now Tokenize the full training and test splits of the datasets, to obtain test_ids and train_ids lists. We will work on these now\n",
    "A good way to proceed is to the '.map' method see https://huggingface.co/docs/datasets/en/process#map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = dataset['training'].map(lambda s: {'ids': tokenize(s['text'], vocab)})['ids']\n",
    "test_ids = dataset['test'].map(lambda s: {'ids': tokenize(s['text'], vocab)})['ids']\n",
    "train_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 We will later on work on **batches** of data as is always the case when training deep learning models.\n",
    "#### The problem here is that we cannot just stack the 'ids' as they have variable length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_ids, batch_size=2, shuffle=True) # produces batch by 'collating' individual samples\n",
    "for elt in dataloader: # will raise an error !\n",
    "    print(elt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To solve this issue, we add a 'padding token' to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '<PAD>' not in vocab:\n",
    "    vocab['<PAD>'] = len(vocab)\n",
    "    pad_id = len(vocab)\n",
    "else:\n",
    "    pad_id = len(vocab) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 implement a 'collate_batch_fn' which pads the sequences found as input using the pad_id, and returns the non-padding mask\n",
    "For instance if input contains\n",
    "[[1, 2, 3], [5, 6, 7, 8]]\n",
    "you should return a torch tensor of type long [[1, 2, 3, pad_id], [5, 6, 7, 8]] as 'ids' and\n",
    "a tensor of type long [[1, 1, 1, 0], [1, 1, 1, 1]] as mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    here batch is a list of list of ints\n",
    "    You should return a dictionary containing two keys;\n",
    "    \"ids\" which contain the padded list of elements in the batch\n",
    "    It should be a torch Tensor of size (len(batch), max([len(elt) for elt in batch]))\n",
    "    \"mask\" which contains zeros where there is padding, same size as 'ids'\n",
    "    \"\"\"\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that this now works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_ids, batch_size=2, shuffle=True, collate_fn=collate_fn) # produces batch by 'collating' individual samples\n",
    "\n",
    "for elt in dataloader: \n",
    "    print(elt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok now we have a dataloader which yields batches of padded tokenized texts. We are ready to start implementing the transformers architecture. We start with the attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention layer\n",
    "Remember the attention layer takes as input a (batched) sequence of hidden states (shape (B, T, H)), and returns a tensor of same dimension exactly containing the attention values.\n",
    "We will start with an example. \n",
    "Let's assume we have this input x to the layer and a head_size of 4 (head_size is dimension of query/keys/values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 7, 16)) # batch  of 2 sequences of 7 hidden states of dimension 16 (B, T, H) = (2, 7, 16)\n",
    "B, T, H = 2, 7, 16\n",
    "head_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Declare key_matrix, query_matrix and values_matrix pytorch layers to produce, from this x, the keys, queries and values. Apply these to get (B, T, head_size) tensors of K, Q, V\n",
    "### We want the values to have shape (B, T, head_size)\n",
    "hint: in torch, matrices are represented as bias-less Linear modules https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_matrix = ...\n",
    "query_matrix = ...\n",
    "value_matrix = ...\n",
    "\n",
    "k = key_matrix(x)\n",
    "q = query_matrix(x)\n",
    "v = value_matrix(x)\n",
    "k.size(), q.size(), v.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Q and K are (B, T, head_size) matrices. To compute the attention scores, we just need to compute the batch matrix multiplication of Q and K.transpose(1, 2): this op will do: (B, T, head_size) x (B, head_size, T) - (B, T, T): one (unnormalized) attention score for each pair of tokens. Implement this using torch.matmul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = ...\n",
    "attention_scores # check the dimensions you obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Now we need to apply the causal masking so that past tokens do not attend future tokens, but future tokens do attend past tokens. To do this, we create a triangular inferior mask for each element of the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ...\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this mask to set to -float('inf') the attention_scores where the mask if 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Apply the softmax to normalize row-wise the masked attention scores. Why did we set attention scores to -float('inf') outside of the mask ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 With another matrix multiplication between the attention_scores (B, T, T) and the values (B, T, head_size), obtain the values as as (B, T, head_size) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now gather it all to create an Attention nn.Module object implementing this attention operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, hidden_state_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.key = ...\n",
    "        self.query = ...\n",
    "        self.value = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return # TODO\n",
    "    \n",
    "attention = Attention(hidden_state_dim=16, head_size = 4) # check it works "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Now create a multi-head attention layer that, given some x, computes n_head attention in parallel, each with a head size of (hidden_state_dim / n_heads). Each head produces a (B, T, head_size) set of values. The multi head attention layer should concatenate them into a (B, T, head_size*n_heads=hidden_dim) and apply a final 'projection' layer which is just a linear map.\n",
    "You can assume that n_heads divide hidden_state_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_state_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([???])\n",
    "        self.proj = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        return # TODO\n",
    "    \n",
    "\n",
    "mha = MultiHeadAttention(16, 4)\n",
    "mha(x).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finshing the transformer block\n",
    "\n",
    "### 4.1: Feed-forward layer (also called MLP layer): Implement the feed forward layer of the transformer block.\n",
    "Reminder: this layer applies two linear layers (embedding_size -> mlp_factor * embedding_size -> embedding size where mlp_factor controls the size of the intermediate layer with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size, mlp_factor=4):\n",
    "        super().__init__()\n",
    "        ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Now implement the full transformer block.\n",
    "For normalization, you can just use:\n",
    "`self.norm = nn.LayerNorm(embedding_size)` and apply it, as any other nn.module, to any input whose last dimension is embedding_size\n",
    "\n",
    "Don't forget the residual connections:\n",
    "- the input x should be added to the output of 1st layer norm + attention to obtain y\n",
    "- y should be added to the output of 2st layer norm + mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_state_dim, n_heads, mlp_factor):\n",
    "        super().__init__()\n",
    "        ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finish implementing the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_state_dim, n_heads, mlp_factor=4):\n",
    "        super().__init__()\n",
    "        ???\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Still things to be done:\n",
    "### 6.1 Implement a 'generate' method which, given a trained Transformer object and an initial list of token ids, generate the following tokens, according to the language model\n",
    "### 6.2 How would you implement the loss function ?\n",
    "### 6.3 Implement positional encodings\n",
    "### 6.4 What else ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
