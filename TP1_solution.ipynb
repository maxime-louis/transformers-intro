{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1072\n",
       "    })\n",
       "    training: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3859\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('notaphoenix/shakespeare_dataset')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['training'][0]\n",
    "\n",
    "\n",
    "list('abcd') #-> ['a', 'b', 'c', 'd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many characters/words are there in total in the train split of the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179380, 40229)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = sum(len(elt['text']) for elt in dataset['training'])\n",
    "n_words = sum(len(elt['text'].split(' ')) for elt in dataset['training'])\n",
    "n_chars, n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How well my comfort is revived by this !\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience we use the training data as a list of str:\n",
    "corpus = [elt['text'] for elt in dataset['training']]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 First step: build a list \"tokenized_corpus\" of same size as the corpus such that tokenized_corpus[i] is the list of characters in corpus[i] (just by splitting the str into its list of characters)\n",
    "So tokenized_corpus starts by representing each corpus entry by a list of characters. As a reminder, Byte-pair encoding will iteratively refine it by merging the most frequent pairs of characters:\n",
    "['a', 'b', 'c', 'a', 'b'] -> ['ab', 'c', 'ab']: this is the representation which we store in 'tokenized_corpus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'v',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " '!',\n",
       " '\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized_corpus = [list(word) for word in corpus] # one-liner (optional)\n",
    "\n",
    "tokenized_corpus = []\n",
    "for elt in corpus:\n",
    "    tokenized_corpus.append(list(elt))\n",
    "\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Now build a dictionary \"vocab\" mapping each unique character in the corpus to a unique integer between 0 and the total number of characters - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('H', 0), ('o', 1), ('w', 2), (' ', 3), ('e', 4), ('l', 5), ('m', 6), ('y', 7), ('c', 8), ('f', 9), ('r', 10), ('t', 11), ('i', 12), ('s', 13), ('v', 14), ('d', 15), ('b', 16), ('h', 17), ('!', 18), ('\\n', 19), ('B', 20), ('u', 21), ('I', 22), ('a', 23), ('k', 24), ('p', 25), (\"'\", 26), (',', 27), ('n', 28), ('.', 29), ('g', 30), ('L', 31), ('O', 32), ('G', 33), ('S', 34), ('A', 35), ('Y', 36), ('C', 37), ('R', 38), ('T', 39), ('W', 40), ('P', 41), ('N', 42), ('?', 43), ('D', 44), ('j', 45), ('q', 46), ('x', 47), ('-', 48), ('F', 49), ('M', 50), ('\"', 51), ('V', 52), ('J', 53), ('E', 54), ('0', 55), ('U', 56), ('z', 57), (';', 58), (':', 59), ('K', 60), (')', 61), ('(', 62), ('Q', 63), ('Z', 64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab = {char: i for i, char in enumerate(set(\"\".join(corpus)))} # one-liner (optional)\n",
    "\n",
    "vocab = {}\n",
    "for elt in tokenized_corpus: # iterate over corpus\n",
    "    for char in elt: # iterate over characters of elt\n",
    "        if char not in vocab: # add to dictionary\n",
    "            vocab[char] = len(vocab) # index of character is current length of vocab !\n",
    "\n",
    "vocab.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do something\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Dictionary manipulations:\n",
    " d = {}\n",
    " d['a'] = 2 # allocation of new key\n",
    " if 'a' in d: # checking a key belongs\n",
    "     print('do something')\n",
    "\n",
    "d['a'] # acccecssing a value (error if 'a' not in d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Now implement a method which, given 'tokenized_corpus' computes a dictionary 'pairs' of which:\n",
    "- the keys are pairs of elements appearing in the corpus (if corpus contains ['c', ..., 'a', 'b', ...] then the tuple ('a', 'b') should appear as key in pairs)\n",
    "- the values are the number of times each pair appears in the corpus\n",
    "\n",
    "(The right way is to build the pairs dictionary by iterating over the tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(('H', 'o'), 80), (('o', 'w'), 609), (('w', ' '), 515), ((' ', 'w'), 1722), (('w', 'e'), 461), (('e', 'l'), 650), (('l', 'l'), 1193), (('l', ' '), 1293), ((' ', 'm'), 2017), (('m', 'y'), 516), (('y', ' '), 2372), ((' ', 'c'), 972), (('c', 'o'), 490), (('o', 'm'), 739), (('m', 'f'), 16), (('f', 'o'), 506), (('o', 'r'), 1106), (('r', 't'), 337), (('t', ' '), 3782), ((' ', 'i'), 1329), (('i', 's'), 1235), (('s', ' '), 3386), ((' ', 'r'), 402), (('r', 'e'), 1626), (('e', 'v'), 188), (('v', 'i'), 179), (('i', 'v'), 244), (('v', 'e'), 1096), (('e', 'd'), 623), (('d', ' '), 2690), ((' ', 'b'), 1422), (('b', 'y'), 150), ((' ', 't'), 3839), (('t', 'h'), 3279), (('h', 'i'), 1123), ((' ', '!'), 402), (('!', '\\n'), 379), (('B', 'u'), 105), (('u', 't'), 596), (('i', 'f'), 197), (('f', ' '), 776), ((' ', 'I'), 761), (('I', ' '), 909), ((' ', 's'), 2087), (('s', 'h'), 474), (('h', 'a'), 1557), (('a', 'k'), 279), (('k', 'e'), 442), (('e', ' '), 6284), (('i', 't'), 882), ((' ', 'u'), 275), (('u', 'p'), 155), (('p', ' '), 213), ((' ', 'a'), 2589), (('a', ' '), 682), ((' ', 'l'), 1038), (('l', 'i'), 584), (('t', 't'), 169), (('t', 'l'), 123), (('l', 'e'), 817), (('t', \"'\"), 187), ((\"'\", 'l'), 223), (('w', 'o'), 320), (('r', 'k'), 46), (('k', ' '), 438), ((' ', ','), 3043), ((',', ' '), 3039), (('b', 'e'), 749), (('e', 'c'), 180), (('c', 'a'), 349), (('a', 'u'), 142), (('u', 's'), 480), (('s', 'e'), 893), ((' ', 'e'), 376), (('e', 'r'), 1761), (('r', 'y'), 222), ((' ', 'o'), 1129), (('o', 'n'), 1062), (('n', 'e'), 621), (('o', 'f'), 510), (('h', 'o'), 753), (('o', 's'), 225), (('e', 't'), 638), (('t', 'e'), 692), (('r', 's'), 331), (('i', 'n'), 1756), (('n', ' '), 2070), ((' ', 'n'), 803), (('n', 'a'), 113), (('a', 'm'), 287), (('m', 'e'), 1282), ((' ', '.'), 2773), (('.', '\\n'), 2773), ((' ', 'k'), 254), (('k', 'n'), 154), (('n', 'o'), 753), (('I', \"'\"), 297), ((\"'\", 'm'), 87), (('m', ' '), 712), (('o', 't'), 549), ((' ', 'f'), 1038), (('o', 'o'), 518), (('o', 'l'), 326), (('n', 'g'), 898), (('g', ' '), 743), (('y', 's'), 72), (('l', 'f'), 87), (('r', ' '), 2116), (('t', 'i'), 436), ((' ', 'g'), 658), (('g', 'e'), 311), (('a', 'r'), 994), (('r', 'r'), 163), (('r', 'i'), 539), (('i', 'e'), 365), (('a', 'w'), 132), (('w', 'a'), 370), (('a', 'y'), 496), (('i', 'm'), 390), (('m', 'a'), 615), (('a', 'g'), 158), (('g', 'i'), 133), (('a', 't'), 1377), (('i', 'o'), 272), (('c', 'l'), 70), (('l', 'u'), 47), (('u', 'e'), 139), ((' ', 'p'), 605), (('p', 'o'), 168), (('o', 'i'), 98), (('n', 't'), 559), (('t', 's'), 144), (('t', 'o'), 1049), (('o', ' '), 1847), (('h', 'e'), 2713), (('f', 'a'), 208), (('a', 'c'), 229), (('c', 't'), 78), ((' ', 'L'), 40), (('L', 'a'), 25), (('a', 'd'), 516), (('d', 'y'), 172), ((' ', 'O'), 73), (('O', 'l'), 25), (('i', 'a'), 138), (('l', 'o'), 566), (('o', 'v'), 271), (('e', 's'), 886), (('I', 'f'), 93), ((' ', 'y'), 1270), (('y', 'o'), 1203), (('o', 'u'), 2463), (('u', ' '), 1056), (('u', 'l'), 416), (('l', 'd'), 357), (('d', 'n'), 39), (('n', \"'\"), 218), ((\"'\", 't'), 239), (('b', 'r'), 144), ((' ', 'h'), 1933), (('b', 'a'), 217), (('c', 'k'), 188), (('f', 'e'), 180), (('G', 'o'), 141), ((' ', 'S'), 113), (('S', 'i'), 86), (('i', 'r'), 435), ((' ', 'A'), 142), (('A', 'n'), 218), (('n', 'd'), 1247), (('d', 'r'), 142), (('e', 'w'), 125), (('Y', 'o'), 152), (('s', 'p'), 119), (('p', 'e'), 255), (('e', 'a'), 1073), (('s', 'o'), 488), (('i', 'g'), 312), (('g', 'h'), 398), (('h', 'l'), 7), (('l', 'y'), 282), (('l', 's'), 80), (('a', 'n'), 1825), (('e', 'y'), 151), ((' ', 'd'), 1000), (('d', 'a'), 225), (('C', 'o'), 86), ((' ', 'R'), 98), (('R', 'o'), 122), (('e', 'o'), 162), (('d', 'o'), 366), (('s', 't'), 836), ((' ', 'T'), 236), (('T', 'o'), 119), (('u', 'r'), 715), (('l', 'a'), 344), ((' ', 'W'), 69), (('W', 'h'), 330), (('o', 'e'), 76), (('e', \"'\"), 231), ((\"'\", 'e'), 19), (('w', 'i'), 551), (('H', 'e'), 182), (('e', 'n'), 1046), (('n', 'c'), 185), (('c', 'e'), 385), (('t', 'a'), 313), (('a', 'l'), 641), (('e', 'e'), 570), (('c', 'h'), 410), (('h', ' '), 978), ((' ', 'P'), 59), (('P', 'u'), 8), (('s', 'i'), 425), ((' ', 'B'), 46), (('B', 'y'), 22), (('r', 'g'), 42), (('f', 'u'), 128), (('O', 'h'), 102), ((' ', 'G'), 48), (('o', 'd'), 272), (('!', ' '), 23), ((' ', 'N'), 42), (('N', 'u'), 35), (('o', 'p'), 107), (('p', 'p'), 104), ((' ', '?'), 702), (('?', '\\n'), 690), (('c', 'r'), 99), (('o', \"'\"), 44), ((\"'\", 's'), 519), (('n', 'i'), 260), (('h', 'm'), 17), (('w', 'h'), 365), (('e', 'i'), 107), (('D', 'o'), 64), (('S', 'e'), 22), (('e', 'b'), 11), (('a', 's'), 745), (('A', 'm'), 7), (('i', 'k'), 121), (('g', 'u'), 79), (('u', 'y'), 12), (('A', 'l'), 24), (('l', 't'), 123), (('u', 'g'), 132), ((' ', 'j'), 92), (('j', 'o'), 50), (('o', 'y'), 61), (('a', 'v'), 371), (('t', 'r'), 233), (('r', 'a'), 347), (('h', 't'), 299), (('g', 'a'), 86), (('m', 'i'), 190), (('e', 'f'), 118), (('d', 'i'), 236), (('i', 'd'), 270), (('d', 's'), 145), (('e', 'q'), 8), (('q', 'u'), 78), (('y', 'e'), 144), (('a', 'i'), 344), (('W', 'e'), 88), (('u', 'n'), 346), (('h', 'u'), 116), (('r', 'c'), 91), (('h', 'y'), 205), (('y', 'a'), 14), (('r', 'd'), 269), (('s', 'k'), 43), (('a', 'b'), 164), (('b', 'o'), 223), (('H', 'a'), 56), (('L', 'e'), 61), (('e', 'p'), 149), (('p', 'i'), 97), (('s', 'u'), 130), (('u', 'c'), 123), (('g', 'r'), 146), (('s', 's'), 270), (('u', 'd'), 49), (('d', 'd'), 32), (('d', 'e'), 507), (('G', 'i'), 29), (('e', 'x'), 75), (('x', 'c'), 49), (('r', 'f'), 22), (('T', 'h'), 388), (('a', 'p'), 145), (('i', 'c'), 241), (('h', 'r'), 55), (('e', '-'), 11), (('-', 'b'), 11), (('r', 'h'), 7), (('y', 't'), 46), (('g', 'o'), 311), (('c', 'i'), 60), (('t', 'w'), 56), (('n', 'n'), 34), (('m', 'o'), 288), (('r', 'n'), 112), (('p', 'u'), 74), ((' ', 'F'), 61), (('F', 'o'), 49), (('L', 'i'), 14), (('f', 'r'), 169), (('r', 'o'), 416), (('s', 'a'), 349), (('d', 'm'), 15), (('r', 'u'), 156), (('p', 'a'), 181), (('M', 'y'), 81), (('d', 'u'), 29), (('t', 'y'), 121), (('u', 'm'), 47), (('m', 'b'), 85), (('b', 'l'), 173), (('r', 'v'), 48), (('i', 'l'), 509), (('n', 'f'), 38), (('s', 'w'), 123), ((' ', \"'\"), 107), (('p', 'r'), 166), (('g', '.'), 1), (('.', ' '), 35), (('o', 'b'), 77), (('e', 'm'), 213), (('w', 'n'), 73), (('P', 'a'), 25), (('o', 'a'), 37), (('b', 'u'), 149), ((' ', 'v'), 136), (('n', 'k'), 153), (('o', 'g'), 33), (('e', 'k'), 22), ((' ', '\"'), 113), (('\"', 'T'), 5), (('-', 'w'), 1), (('l', 'c'), 18), ((' ', 'V'), 20), (('V', 'i'), 12), (('\"', ' '), 100), ((' ', 'M'), 124), (('I', 't'), 74), (('S', 'w'), 10), (('t', 'c'), 44), (('p', 'l'), 156), (('n', 'y'), 119), (('O', 'r'), 34), (('k', 'i'), 146), (('t', 'u'), 132), (('u', 'o'), 9), (('b', 's'), 22), (('s', '-'), 7), (('-', 'g'), 11), (('a', 'q'), 2), (('u', 'a'), 40), (('a', 'e'), 2), (('d', 'w'), 4), ((' ', 'J'), 45), (('J', 'u'), 47), (('y', 'i'), 42), (('B', 'e'), 59), (('O', 'f'), 9), (('e', 'h'), 25), (('n', 'j'), 7), (('w', 'r'), 69), (('e', 'g'), 64), (('P', 'e'), 20), (('Y', 'e'), 43), (('l', 'g'), 6), (('i', 'p'), 40), (('p', 's'), 43), (('y', \"'\"), 26), ((\"'\", 'r'), 89), (('s', 'd'), 22), ((' ', 'H'), 27), ((\"'\", 'd'), 40), (('v', 'o'), 79), (('p', 'y'), 29), ((' ', 'q'), 54), (('I', 's'), 45), (('s', 'n'), 29), (('M', 'o'), 44), (('T', 'y'), 61), (('y', 'b'), 74), (('S', 'h'), 66), (('N', 'o'), 122), (('D', 'e'), 25), (('w', 'l'), 13), (('F', 'r'), 29), ((' ', 'E'), 25), (('E', 'a'), 5), (('h', \"'\"), 4), (('b', ' '), 40), (('f', 'f'), 55), (('r', 'm'), 40), (('m', 's'), 39), (('g', 'n'), 29), (('h', 's'), 11), (('a', 'f'), 50), (('P', 'r'), 17), (('S', 'l'), 3), ((\"'\", ' '), 49), (('y', 'f'), 9), (('g', 's'), 58), (('r', 'l'), 74), (('n', 's'), 216), (('s', 'm'), 35), (('D', 'i'), 15), (('c', 'u'), 78), (('S', 'a'), 20), (('o', 'k'), 120), (('f', 'y'), 6), (('n', '-'), 7), (('-', 'd'), 6), (('V', 'e'), 16), (('u', \"'\"), 109), (('A', 'w'), 7), ((\"'\", 'c'), 4), (('o', 'c'), 46), (('f', 'i'), 151), (('f', 'l'), 46), (('s', 'y'), 13), (('n', 'r'), 7), (('w', \"'\"), 9), (('A', 's'), 25), (('r', \"'\"), 37), (('d', 'b'), 9), (('d', 'v'), 9), (('B', 'o'), 6), ((' ', 'C'), 75), (('C', 'u'), 9), (('d', \"'\"), 13), (('m', 'm'), 27), (('m', 'u'), 116), (('u', 'f'), 4), (('c', 'c'), 23), (('p', 't'), 50), (('n', 'l'), 55), (('j', 'u'), 40), (('G', 'r'), 7), (('s', 'l'), 61), (('C', 'a'), 42), (('j', 'e'), 16), ((' ', 'Y'), 7), (('C', 'e'), 22), (('v', 'a'), 44), (('E', 'r'), 2), (('s', 'b'), 22), (('m', 'p'), 62), ((' ', '0'), 1), (('0', 'a'), 1), (('l', \"'\"), 10), (('M', 'a'), 111), (('l', 'v'), 49), (('h', 'w'), 2), ((' ', 'U'), 10), (('U', 'n'), 10), (('i', 'z'), 11), (('z', 'e'), 18), (('d', '.'), 11), (('T', 'a'), 30), (('A', 'r'), 33), (('b', 'i'), 79), (('W', 'o'), 14), (('l', 'k'), 34), (('n', 'b'), 2), (('u', 'i'), 69), ((' ', 'D'), 22), (('f', 't'), 51), ((' ', ';'), 21), ((';', ' '), 20), (('W', 'i'), 35), ((\"'\", 'h'), 1), (('d', 'l'), 41), (('y', '.'), 6), (('L', 'o'), 33), (('g', 'g'), 30), (('s', 'c'), 52), (('A', ' '), 57), (('g', 'm'), 3), (('F', 'a'), 47), (('O', 'u'), 19), (('m', 'n'), 12), (('y', 'm'), 9), (('I', 'n'), 33), (('t', 'n'), 17), ((\"'\", 'v'), 42), (('C', 'h'), 10), (('i', 'b'), 23), (('F', 'l'), 3), (('S', 't'), 22), (('i', ' '), 14), (('o', 'z'), 7), (('A', 't'), 5), (('x', 'p'), 5), (('a', 'z'), 21), (('z', 'z'), 4), (('z', 'l'), 3), (('k', 'y'), 8), (('O', ' '), 54), (('-', 'i'), 1), (('-', 'l'), 3), (('w', 'f'), 5), (('H', 'u'), 8), (('R', 'u'), 5), (('h', 'q'), 1), (('w', 's'), 63), (('k', 's'), 48), (('y', 'l'), 9), (('l', 'w'), 3), (('e', '.'), 17), (('U', 's'), 1), (('u', 'b'), 27), (('O', 'n'), 18), (('x', 't'), 17), (('P', 'o'), 9), (('c', ' '), 26), (('y', 'r'), 16), (('n', 'v'), 22), (('k', 'l'), 20), ((' ', ':'), 14), ((':', ' '), 14), (('T', 'e'), 21), (('S', 'u'), 13), (('T', 'w'), 9), (('r', 'b'), 13), (('c', 's'), 2), (('h', '.'), 2), (('s', 'g'), 7), (('t', '.'), 4), (('r', 'w'), 6), (('\"', 'R'), 3), (('.', '\"'), 46), (('A', 'y'), 23), (('D', 'a'), 5), (('l', 'm'), 17), (('S', 'o'), 58), (('g', '-'), 4), (('-', 's'), 9), (('r', 'p'), 11), (('M', 'e'), 30), (('n', 'x'), 2), (('x', ' '), 8), (('l', 'r'), 25), (('D', 'r'), 3), (('p', 'h'), 20), (('R', 'e'), 10), (('n', 'u'), 21), (('H', 'i'), 20), (('T', 'i'), 23), (('J', 'o'), 7), (('k', 'f'), 4), (('x', 'i'), 12), (('n', 'z'), 1), (('z', 'y'), 12), (('l', 'p'), 23), (('E', 'n'), 3), (('v', 'y'), 4), (('e', 'u'), 9), (('L', 'u'), 4), (('t', '-'), 8), (('-', 'c'), 11), (('p', 'b'), 1), (('n', 'h'), 13), (('h', 'f'), 6), (('P', 'l'), 20), (('o', '.'), 7), (('G', 'e'), 21), (('D', 'u'), 3), (('u', 'k'), 8), (('b', 'b'), 10), (('A', 'h'), 17), (('E', 'd'), 1), (('c', 'y'), 14), (('?', ' '), 12), (('s', 'f'), 11), (('E', 'v'), 22), (('O', 'b'), 2), (('d', 'g'), 8), (('d', 't'), 3), (('\"', 'p'), 1), (('d', '\"'), 5), (('\"', 'I'), 4), (('\"', 'n'), 4), (('u', '\"'), 1), (('g', \"'\"), 7), (('g', 'l'), 19), (('\"', 'W'), 2), (('e', '\"'), 5), (('U', 'p'), 7), (('m', 't'), 3), (('N', 'e'), 12), (('W', 'a'), 11), (('N', 'a'), 18), (('z', 'i'), 7), (('s', \"'\"), 2), (('V', 'u'), 1), (('d', '-'), 7), (('-', 'h'), 8), (('t', 'f'), 1), (('m', 'w'), 2), (('t', 'p'), 2), (('a', \"'\"), 9), (('I', 'l'), 13), (('o', 'x'), 11), (('R', '.'), 1), ((' ', 'K'), 2), (('K', 'n'), 2), (('\"', 'H'), 3), (('a', 'h'), 11), (('\"', 'a'), 6), (('t', '\"'), 3), (('M', 'i'), 11), (('r', '-'), 8), (('-', 'u'), 1), (('z', ' '), 5), (('v', 'u'), 2), (('a', 'x'), 4), (('x', 'a'), 4), (('K', 'e'), 4), (('C', 'r'), 4), (('x', 'q'), 3), (('n', 'm'), 4), (('A', 'c'), 5), (('c', 'q'), 3), (('\"', 'G'), 1), (('M', 'u'), 6), (('T', 'u'), 2), (('B', 'r'), 7), (('\"', 's'), 6), (('m', '.'), 3), (('y', 'w'), 6), (('-', 'j'), 2), (('w', '-'), 4), (('i', 'u'), 3), (('a', '-'), 4), (('o', 'h'), 4), (('\"', 'N'), 2), (('w', '.'), 7), (('\"', '\\n'), 7), (('\"', 'b'), 6), (('y', '\"'), 4), (('n', 'p'), 3), ((',', '\\n'), 5), (('F', 'i'), 6), (('x', 'u'), 1), (('R', 'i'), 3), (('A', 'b'), 5), (('A', 'p'), 5), (('\"', 'F'), 2), (('b', 'm'), 3), (('J', 'a'), 4), (('M', '\"'), 1), (('g', 't'), 2), (('h', 'p'), 1), (('l', '.'), 3), ((\"'\", 'n'), 2), (('p', 'm'), 1), (('f', \"'\"), 2), (('b', 'n'), 2), (('u', 'v'), 1), (('V', 'a'), 3), ((';', '\\n'), 1), (('l', '-'), 9), (('-', 'a'), 4), (('y', 'p'), 3), (('s', '.'), 10), (('p', '-'), 2), (('k', '-'), 1), (('-', 'p'), 3), (('\"', 'o'), 2), (('r', '.'), 3), (('S', 'p'), 8), (('l', 'n'), 2), (('e', 'j'), 3), (('i', 'x'), 6), (('x', 'e'), 6), (('U', 't'), 1), (('p', \"'\"), 1), (('f', '-'), 1), (('y', '-'), 5), (('-', 't'), 2), (('H', 'm'), 1), (('O', 'p'), 2), (('M', 'r'), 3), (('E', 'y'), 1), (('\"', 'e'), 1), (('E', 'x'), 8), (('A', 'f'), 2), (('h', 'c'), 1), (('F', 'u'), 2), (('o', 'q'), 2), (('B', 'l'), 5), (('t', 'z'), 1), (('n', 'w'), 4), (('h', 'n'), 3), (('\"', 'f'), 3), (('\"', 'g'), 1), (('O', 'd'), 1), (('n', 'q'), 2), (('g', 'y'), 6), (('N', 'i'), 3), (('u', 'z'), 3), (('R', 'a'), 2), (('o', '-'), 2), (('\"', 'D'), 1), (('E', 'g'), 2), (('h', '-'), 6), (('-', 'r'), 3), ((' ', ')'), 5), ((')', ' '), 5), (('a', '\\n'), 1), ((\"'\", 'w'), 1), (('b', 't'), 2), (('B', 'a'), 3), (('\"', 'Y'), 2), (('-', 'o'), 1), (('A', 'u'), 1), (('\"', 'd'), 4), (('\"', 'A'), 2), (('\"', 'l'), 2), (('K', 'i'), 1), (('t', 'm'), 3), (('C', 'l'), 3), (('c', \"'\"), 2), (('s', 'q'), 2), (('E', 'l'), 5), (('P', 'i'), 2), (('f', 's'), 2), (('r', '\\n'), 1), (('S', 'c'), 1), (('m', '-'), 1), ((' ', '('), 4), (('(', ' '), 4), (('\"', 'm'), 2), (('o', 'j'), 1), (('Q', 'u'), 1), (('-', 'e'), 1), (('P', 'y'), 3), (('d', 'f'), 1), (('B', 'i'), 1), (('b', 'h'), 1), (('a', ','), 1), (('d', 'p'), 2), (('k', 'w'), 1), (('h', 'd'), 2), (('F', 'e'), 6), (('a', '.'), 2), ((\"'\", 'W'), 1), (('\"', 'C'), 1), (('r', 'z'), 1), (('n', '.'), 2), (('T', 's'), 1), (('T', 'r'), 3), (('\"', 'M'), 3), (('b', 'j'), 1), (('-', 'n'), 2), (('A', 'g'), 4), (('k', '\"'), 2), (('p', 'd'), 1), (('d', '\\n'), 1), (('-', 'm'), 3), (('Z', 'o'), 1), (('S', 'm'), 2), (('\"', 'L'), 1), (('d', 'h'), 1), (('k', \"'\"), 2), (('w', 'k'), 1), (('E', 'i'), 5), (('u', '.'), 1), (('\"', 'y'), 1), (('t', '\\n'), 1), (('k', 'a'), 3), (('E', 'c'), 1), (('\"', 'P'), 1), (('y', 'v'), 1), (('-', 'f'), 2), ((\"'\", 'T'), 1), (('T', ' '), 1), (('M', '.'), 2), (('.', 'O'), 2), (('O', '.'), 2), (('.', 'A'), 2), (('A', '.'), 2), (('.', 'I'), 2), (('I', '.'), 1), (('\"', 'S'), 1), (('-', ' '), 1), (('F', 'O'), 1), (('O', 'O'), 1), (('O', 'L'), 1), (('L', ' '), 1), (('-', 'q'), 1), (('z', 'o'), 1), (('h', 'b'), 1), (('A', 'd'), 1), (('x', 'w'), 1), (('s', 'r'), 1), (('d', 'c'), 1), (('W', 'r'), 1), (('e', 'z'), 1)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pairs(tokenized_corpus):\n",
    "    \"\"\"Get the frequency of adjacent pairs in the words.\"\"\"\n",
    "    pairs = {}\n",
    "    for word in tokenized_corpus:\n",
    "        for i in range(len(word) - 1):\n",
    "            if (word[i], word[i + 1]) not in pairs:\n",
    "                pairs[(word[i], word[i + 1])] = 1\n",
    "            else:\n",
    "                pairs[(word[i], word[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "pairs = get_pairs(tokenized_corpus)\n",
    "pairs.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, given pairs, we can compute the most frequent pair of tokens via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_to_merge = max(pairs, key=pairs.get)\n",
    "pair_to_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Write a method which, given a pair_to_merge and the current tokenized_corpus and applies the merge operation to the tokenized_corpus\n",
    "e.g. if the pair to merge is ('a', 'b') and some entry in tokenized_corpus contains ['c', ..., 'a', 'b'...], it should be mapped to ['c', ..., 'ab', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'o', 'w', ' ', 'w', 'e', 'l', 'l', ' ', 'm', 'y', ' ', 'c', 'o', 'm', 'f', 'o', 'r', 't', ' ', 'i', 's', ' ', 'r', 'e', 'v', 'i', 'v', 'e', 'd', ' ', 'b', 'y', ' ', 't', 'h', 'i', 's', ' ', '!', '\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ho',\n",
       " 'w',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'v',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " '!',\n",
       " '\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_pair(pair, tokenized_corpus):\n",
    "    \"\"\"Merge the most frequent pair in all tokenized_corpus.\"\"\"\n",
    "    new_words = []\n",
    "    for word in tokenized_corpus:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                new_word.append(word[i] + word[i + 1])  # Merge pair (\"+\" is str concatenation)\n",
    "                i += 2  # Skip next character since it's merged\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "tokenized_corpus = merge_pair(pair_to_merge, tokenized_corpus)\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Now write the full byte-pair encoding algorithm which, given an initial corpus (list of str) returns its tokenized version as well as the vocabulary.\n",
    "This function should take a parameter 'n_merge' indicating the number of merges we will do. \n",
    "\n",
    "What is the final length of the vocabulary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_pair_encoding(corpus, num_merges: int = 10):\n",
    "    \"\"\"Perform BPE on a given corpus.\"\"\"\n",
    "    tokenized_corpus = [list(word) for word in corpus]  # Start with character tokens\n",
    "    vocab = {char: i for i, char in enumerate(set(\"\".join(corpus)))}  # Initial vocab\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_pairs(tokenized_corpus)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        tokenized_corpus = merge_pair(best_pair, tokenized_corpus)\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[new_token] = len(vocab)  # Assign new token an index\n",
    "    \n",
    "    return vocab, tokenized_corpus\n",
    "\n",
    "vocab, tokenized_corpus = byte_pair_encoding(corpus, num_merges=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Now, given the obtained 'vocab' write a 'tokenize_to_str_list' method which applies the tokenization to an input string s, returns the list of tokens as a list of str\n",
    "Verify your result on dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L',\n",
       " 'ea',\n",
       " 've ',\n",
       " 'me ',\n",
       " 'a',\n",
       " 'lo',\n",
       " 'ne',\n",
       " ' ',\n",
       " 'for ',\n",
       " 'a ',\n",
       " 'min',\n",
       " 'u',\n",
       " 'te ',\n",
       " '.\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_to_str_list(s: str, vocab):\n",
    "    \"\"\"Tokenize a given string based on the trained BPE vocabulary.\"\"\"\n",
    "    tokens = list(s)  # Start with character tokens\n",
    "    \n",
    "    while True:\n",
    "        pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "        valid_pairs = [pair for pair in pairs if pair[0] + pair[1] in vocab]\n",
    "        \n",
    "        if not valid_pairs:\n",
    "            break\n",
    "        \n",
    "        best_pair = max(valid_pairs, key=lambda p: vocab.get(p[0] + p[1], float('-inf')))\n",
    "        merged_token = best_pair[0] + best_pair[1]\n",
    "        \n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                new_tokens.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        tokens = new_tokens\n",
    "    \n",
    "    return tokens    \n",
    "\n",
    "tokenize_to_str_list(dataset['test'][0]['text'], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Now, given the obtained 'vocab' write a 'tokenize' method which applies the tokenization to an input string s, returns the list of integers corresponding to each symbol (the keys of vocab!)\n",
    "Verify your result on dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 81, 98, 88, 33, 165, 283, 59, 163, 100, 427, 51, 296, 70]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s: str, vocab):\n",
    "    tokens = tokenize_to_str_list(s, vocab)\n",
    "    return [vocab[elt] for elt in tokens]\n",
    "\n",
    "tokenize(dataset['test'][0]['text'], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => Yay, we are not able to map any input text to list of integers, not too long, not too short, of which representatens are not too rare, and which preserves (at least partially) the structure of the words\n",
    "### 1.8 What are the current limitations of our implementation of BPE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multiple redundant tokenizations e.g. dog! and dog?\n",
    "- we need to handle unknown characters to not run into errors later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
